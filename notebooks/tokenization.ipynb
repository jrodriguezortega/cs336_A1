{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce855a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0783edb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 72: H\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('H', 72)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "idx = 72\n",
    "print(f\"idx {idx}: {chr(idx)}\")\n",
    "chr(idx), ord(chr(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14499120",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 5 found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;28mchr\u001b[39m(\u001b[32m76\u001b[39m)), \u001b[38;5;28;43mord\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlambd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: ord() expected a character, but string of length 5 found"
     ]
    }
   ],
   "source": [
    "ord(chr(76)), ord(\"lambd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe246b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello world! I am Jos\\xc3\\xa9.'\n",
      "[72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33, 32, 73, 32, 97, 109, 32, 74, 111, 115, 195, 169, 46]\n",
      "b'\\xff\\xfeH\\x00e\\x00l\\x00l\\x00o\\x00 \\x00w\\x00o\\x00r\\x00l\\x00d\\x00!\\x00 \\x00I\\x00 \\x00a\\x00m\\x00 \\x00J\\x00o\\x00s\\x00\\xe9\\x00.\\x00'\n",
      "[255, 254, 72, 0, 101, 0, 108, 0, 108, 0, 111, 0, 32, 0, 119, 0, 111, 0, 114, 0, 108, 0, 100, 0, 33, 0, 32, 0, 73, 0, 32, 0, 97, 0, 109, 0, 32, 0, 74, 0, 111, 0, 115, 0, 233, 0, 46, 0]\n",
      "b'\\xff\\xfe\\x00\\x00H\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00 \\x00\\x00\\x00w\\x00\\x00\\x00o\\x00\\x00\\x00r\\x00\\x00\\x00l\\x00\\x00\\x00d\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00I\\x00\\x00\\x00 \\x00\\x00\\x00a\\x00\\x00\\x00m\\x00\\x00\\x00 \\x00\\x00\\x00J\\x00\\x00\\x00o\\x00\\x00\\x00s\\x00\\x00\\x00\\xe9\\x00\\x00\\x00.\\x00\\x00\\x00'\n",
      "[255, 254, 0, 0, 72, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 32, 0, 0, 0, 119, 0, 0, 0, 111, 0, 0, 0, 114, 0, 0, 0, 108, 0, 0, 0, 100, 0, 0, 0, 33, 0, 0, 0, 32, 0, 0, 0, 73, 0, 0, 0, 32, 0, 0, 0, 97, 0, 0, 0, 109, 0, 0, 0, 32, 0, 0, 0, 74, 0, 0, 0, 111, 0, 0, 0, 115, 0, 0, 0, 233, 0, 0, 0, 46, 0, 0, 0]\n",
      "b'Hello world! I am Jos\\xc3\\xa9.'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bytes must be in range(0, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mbytes\u001b[39m(list_utf_8))\n\u001b[32m     16\u001b[39m list_utf_8[\u001b[32m0\u001b[39m] = \u001b[32m2000\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlist_utf_8\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mValueError\u001b[39m: bytes must be in range(0, 256)"
     ]
    }
   ],
   "source": [
    "# Comparison between UTF-8, UTF-16 and UTF-32 encondings\n",
    "string = \"Hello world! I am José.\"\n",
    "\n",
    "utf_8 = string.encode(\"utf-8\")\n",
    "utf_16 = string.encode(\"utf-16\")\n",
    "utf_32 = string.encode(\"utf-32\")\n",
    "list_utf_8 = list(utf_8)\n",
    "print(utf_8)\n",
    "print(list(utf_8))\n",
    "print(utf_16)\n",
    "print(list(utf_16))\n",
    "print(utf_32)\n",
    "print(list(utf_32))\n",
    "\n",
    "print(bytes(list_utf_8))\n",
    "list_utf_8[0] = 2000\n",
    "print(bytes(list_utf_8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676d2829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([72,\n",
       "  101,\n",
       "  108,\n",
       "  108,\n",
       "  111,\n",
       "  32,\n",
       "  119,\n",
       "  111,\n",
       "  114,\n",
       "  108,\n",
       "  100,\n",
       "  33,\n",
       "  32,\n",
       "  73,\n",
       "  32,\n",
       "  97,\n",
       "  109,\n",
       "  32,\n",
       "  74,\n",
       "  111,\n",
       "  115,\n",
       "  195,\n",
       "  169],\n",
       " b'Hello world! I am Jos\\xc3\\xa9')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Hello world! I am José\"\n",
    "encode = string.encode(encoding=\"utf-8\")\n",
    "\n",
    "list(encode), encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d733329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([72,\n",
       "  101,\n",
       "  108,\n",
       "  108,\n",
       "  111,\n",
       "  32,\n",
       "  119,\n",
       "  111,\n",
       "  114,\n",
       "  108,\n",
       "  100,\n",
       "  33,\n",
       "  32,\n",
       "  73,\n",
       "  32,\n",
       "  97,\n",
       "  109,\n",
       "  32,\n",
       "  74,\n",
       "  111,\n",
       "  115,\n",
       "  195,\n",
       "  161],\n",
       " b'Hello world! I am Jos\\xc3\\xa1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Hello world! I am Josá\"\n",
    "encode = string.encode(encoding=\"utf-8\")\n",
    "\n",
    "list(encode), encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76078a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! I am JosÃ¡\n",
      "Hello world! I am Josá\n"
     ]
    }
   ],
   "source": [
    "final_string = \"\"\n",
    "for idx in encode:\n",
    "    final_string += chr(idx)\n",
    "\n",
    "print(final_string)\n",
    "print(encode.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e8f646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 72)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string[0], encode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32587feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, defaultdict(int, {(1, 1): 0}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = defaultdict(int)\n",
    "\n",
    "counts[(1,1)], counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0ebefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 119,\n",
       " 111,\n",
       " 114,\n",
       " 108,\n",
       " 100,\n",
       " 33,\n",
       " 32,\n",
       " 73,\n",
       " 32,\n",
       " 97,\n",
       " 109,\n",
       " 32,\n",
       " 74,\n",
       " 111,\n",
       " 115,\n",
       " 195,\n",
       " 161]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, encode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3617fac",
   "metadata": {},
   "source": [
    "## Initial vocabulary initialization\n",
    "\n",
    "Since we are using the \"byte-level\" version of the byte pair tokenizer, we start our vocabulary with the 256 codes that a byte can represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a36a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 14,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 62,\n",
       " 63: 63,\n",
       " 64: 64,\n",
       " 65: 65,\n",
       " 66: 66,\n",
       " 67: 67,\n",
       " 68: 68,\n",
       " 69: 69,\n",
       " 70: 70,\n",
       " 71: 71,\n",
       " 72: 72,\n",
       " 73: 73,\n",
       " 74: 74,\n",
       " 75: 75,\n",
       " 76: 76,\n",
       " 77: 77,\n",
       " 78: 78,\n",
       " 79: 79,\n",
       " 80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 86,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99,\n",
       " 100: 100,\n",
       " 101: 101,\n",
       " 102: 102,\n",
       " 103: 103,\n",
       " 104: 104,\n",
       " 105: 105,\n",
       " 106: 106,\n",
       " 107: 107,\n",
       " 108: 108,\n",
       " 109: 109,\n",
       " 110: 110,\n",
       " 111: 111,\n",
       " 112: 112,\n",
       " 113: 113,\n",
       " 114: 114,\n",
       " 115: 115,\n",
       " 116: 116,\n",
       " 117: 117,\n",
       " 118: 118,\n",
       " 119: 119,\n",
       " 120: 120,\n",
       " 121: 121,\n",
       " 122: 122,\n",
       " 123: 123,\n",
       " 124: 124,\n",
       " 125: 125,\n",
       " 126: 126,\n",
       " 127: 127,\n",
       " 128: 128,\n",
       " 129: 129,\n",
       " 130: 130,\n",
       " 131: 131,\n",
       " 132: 132,\n",
       " 133: 133,\n",
       " 134: 134,\n",
       " 135: 135,\n",
       " 136: 136,\n",
       " 137: 137,\n",
       " 138: 138,\n",
       " 139: 139,\n",
       " 140: 140,\n",
       " 141: 141,\n",
       " 142: 142,\n",
       " 143: 143,\n",
       " 144: 144,\n",
       " 145: 145,\n",
       " 146: 146,\n",
       " 147: 147,\n",
       " 148: 148,\n",
       " 149: 149,\n",
       " 150: 150,\n",
       " 151: 151,\n",
       " 152: 152,\n",
       " 153: 153,\n",
       " 154: 154,\n",
       " 155: 155,\n",
       " 156: 156,\n",
       " 157: 157,\n",
       " 158: 158,\n",
       " 159: 159,\n",
       " 160: 160,\n",
       " 161: 161,\n",
       " 162: 162,\n",
       " 163: 163,\n",
       " 164: 164,\n",
       " 165: 165,\n",
       " 166: 166,\n",
       " 167: 167,\n",
       " 168: 168,\n",
       " 169: 169,\n",
       " 170: 170,\n",
       " 171: 171,\n",
       " 172: 172,\n",
       " 173: 173,\n",
       " 174: 174,\n",
       " 175: 175,\n",
       " 176: 176,\n",
       " 177: 177,\n",
       " 178: 178,\n",
       " 179: 179,\n",
       " 180: 180,\n",
       " 181: 181,\n",
       " 182: 182,\n",
       " 183: 183,\n",
       " 184: 184,\n",
       " 185: 185,\n",
       " 186: 186,\n",
       " 187: 187,\n",
       " 188: 188,\n",
       " 189: 189,\n",
       " 190: 190,\n",
       " 191: 191,\n",
       " 192: 192,\n",
       " 193: 193,\n",
       " 194: 194,\n",
       " 195: 195,\n",
       " 196: 196,\n",
       " 197: 197,\n",
       " 198: 198,\n",
       " 199: 199,\n",
       " 200: 200,\n",
       " 201: 201,\n",
       " 202: 202,\n",
       " 203: 203,\n",
       " 204: 204,\n",
       " 205: 205,\n",
       " 206: 206,\n",
       " 207: 207,\n",
       " 208: 208,\n",
       " 209: 209,\n",
       " 210: 210,\n",
       " 211: 211,\n",
       " 212: 212,\n",
       " 213: 213,\n",
       " 214: 214,\n",
       " 215: 215,\n",
       " 216: 216,\n",
       " 217: 217,\n",
       " 218: 218,\n",
       " 219: 219,\n",
       " 220: 220,\n",
       " 221: 221,\n",
       " 222: 222,\n",
       " 223: 223,\n",
       " 224: 224,\n",
       " 225: 225,\n",
       " 226: 226,\n",
       " 227: 227,\n",
       " 228: 228,\n",
       " 229: 229,\n",
       " 230: 230,\n",
       " 231: 231,\n",
       " 232: 232,\n",
       " 233: 233,\n",
       " 234: 234,\n",
       " 235: 235,\n",
       " 236: 236,\n",
       " 237: 237,\n",
       " 238: 238,\n",
       " 239: 239,\n",
       " 240: 240,\n",
       " 241: 241,\n",
       " 242: 242,\n",
       " 243: 243,\n",
       " 244: 244,\n",
       " 245: 245,\n",
       " 246: 246,\n",
       " 247: 247,\n",
       " 248: 248,\n",
       " 249: 249,\n",
       " 250: 250,\n",
       " 251: 251,\n",
       " 252: 252,\n",
       " 253: 253,\n",
       " 254: 254,\n",
       " 255: 255}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {idx: idx for idx in range(256)}\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33747db9",
   "metadata": {},
   "source": [
    "## Pre-tokenization\n",
    "\n",
    "If we simply apply the byte-level BPE algorithm we may encode words with highly similar semantic meaning. For example, the sequences \"dog!\" and \"dog?\" can be represented with two different tokens even tough the have high semantic similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9381677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']\n",
      "['<|', 'endoftext', '|>', ' Once', ' you', ' have', ' a', ' vocabulary', ',', ' you', ' could', ',', ' in', ' principle', ',', ' count', ' how', ' often', ' bytes', ' occur', ' next', ' to', ' each', ' other', ' in', ' your', ' text', ' and', ' begin', ' merging', ' them', ' starting', ' with', ' the', ' most', ' frequent', ' pair', ' of', ' bytes', '.', ' However', ',', ' this', ' is', ' quite', ' computationally', ' expensive', ',', ' since', ' we', '’', 'd', ' have', ' to', ' go', ' take', ' a', ' full', ' pass', ' over', ' the', ' corpus', ' each', ' time', ' we', ' merge', '.', ' In', ' addition', ',', ' directly', ' merging', ' bytes', ' across', ' the', ' corpus', ' may', ' result', ' in', ' tokens', ' that', ' differ', ' only', ' in', ' punctuation', ' <|', 'endoftext', '|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_regex.Scanner at 0x110015230>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "long_text = \"<|endoftext|> Once you have a vocabulary, you could, in principle, count how often bytes occur next \\\n",
    "to each other in your text and begin merging them starting with the most frequent pair of bytes. However, \\\n",
    "this is quite computationally expensive, since we’d have to go take a full pass over the corpus each time \\\n",
    "we merge. In addition, directly merging bytes across the corpus may result in tokens that differ only in \\\n",
    "punctuation <|endoftext|>\"\n",
    "\n",
    "print(re.findall(PAT, \"some text that i'll pre-tokenize\"))\n",
    "\n",
    "print(re.findall(PAT, long_text))\n",
    "\n",
    "re.finditer(PAT, long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f5dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_counts = defaultdict(int)\n",
    "\n",
    "# TODO: Paralellize the pre-tokenization with a map-reduce approach.\n",
    "# Each sub-process will give you the count for its batch and the reduce will sum repeated pre-tokens found in the different batches.\n",
    "# NOTE: The split for the batches must be done in a smart way. E.G. spliting by \\n or something. Not just cut text arbitrarily.\n",
    "for match in re.finditer(PAT, long_text):\n",
    "    string = match.group(0)\n",
    "    string_counts[string] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc2a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', '|']\n",
      "[60, 124]\n"
     ]
    }
   ],
   "source": [
    "string_count_keys = list(string_counts.keys())\n",
    "\n",
    "count_keys_bytes = [key.encode(\"utf-8\") for key in string_count_keys]\n",
    "\n",
    "print(list(string_count_keys[0]))\n",
    "print(list(count_keys_bytes[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a61c3e",
   "metadata": {},
   "source": [
    "## Compute the BPE merges\n",
    "\n",
    "After doing the pre-tokenization and representing each pre-token as a sequence of bytes, we can start doing the merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbfbcc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('<', '|'): 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_byte_pairs(pretoken: bytes, num_appaerances: int):\n",
    "    \"\"\"Compute the count of byte-pairs for a byte sequence of a pre-token\n",
    "\n",
    "    Args:\n",
    "        bytes_seq (list(byte)): Sequence of bytes of a pre-token\n",
    "        num_appaerances (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    print(pretoken)\n",
    "    counts = defaultdict(int)\n",
    "    for byte_1, byte_2 in zip(pretoken[:-1], pretoken[1:]):\n",
    "        counts[(byte_1, byte_2)] += num_appaerances\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "count_byte_pairs(string_count_keys[0], string_counts[string_count_keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132dd8a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1192719647.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmerges =\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = {idx: idx for idx in range(256)}\n",
    "merges = \n",
    "global_counts = defaultdict(int)\n",
    "not_finished = True\n",
    "\n",
    "# Get the initial byte pairs count\n",
    "for string_key in string_count_keys:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fa3579f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x01\\xff'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes([0,1,255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe65e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"<|endoftext|>\"]\n",
    "\n",
    "\"|\".join(special_tokens)\n",
    "\n",
    "print(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5a32a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(bytes,\n",
       "             {0: b'\\x00',\n",
       "              1: b'\\x01',\n",
       "              2: b'\\x02',\n",
       "              3: b'\\x03',\n",
       "              4: b'\\x04',\n",
       "              5: b'\\x05',\n",
       "              6: b'\\x06',\n",
       "              7: b'\\x07',\n",
       "              8: b'\\x08',\n",
       "              9: b'\\t',\n",
       "              10: b'\\n',\n",
       "              11: b'\\x0b',\n",
       "              12: b'\\x0c',\n",
       "              13: b'\\r',\n",
       "              14: b'\\x0e',\n",
       "              15: b'\\x0f',\n",
       "              16: b'\\x10',\n",
       "              17: b'\\x11',\n",
       "              18: b'\\x12',\n",
       "              19: b'\\x13',\n",
       "              20: b'\\x14',\n",
       "              21: b'\\x15',\n",
       "              22: b'\\x16',\n",
       "              23: b'\\x17',\n",
       "              24: b'\\x18',\n",
       "              25: b'\\x19',\n",
       "              26: b'\\x1a',\n",
       "              27: b'\\x1b',\n",
       "              28: b'\\x1c',\n",
       "              29: b'\\x1d',\n",
       "              30: b'\\x1e',\n",
       "              31: b'\\x1f',\n",
       "              32: b' ',\n",
       "              33: b'!',\n",
       "              34: b'\"',\n",
       "              35: b'#',\n",
       "              36: b'$',\n",
       "              37: b'%',\n",
       "              38: b'&',\n",
       "              39: b\"'\",\n",
       "              40: b'(',\n",
       "              41: b')',\n",
       "              42: b'*',\n",
       "              43: b'+',\n",
       "              44: b',',\n",
       "              45: b'-',\n",
       "              46: b'.',\n",
       "              47: b'/',\n",
       "              48: b'0',\n",
       "              49: b'1',\n",
       "              50: b'2',\n",
       "              51: b'3',\n",
       "              52: b'4',\n",
       "              53: b'5',\n",
       "              54: b'6',\n",
       "              55: b'7',\n",
       "              56: b'8',\n",
       "              57: b'9',\n",
       "              58: b':',\n",
       "              59: b';',\n",
       "              60: b'<',\n",
       "              61: b'=',\n",
       "              62: b'>',\n",
       "              63: b'?',\n",
       "              64: b'@',\n",
       "              65: b'A',\n",
       "              66: b'B',\n",
       "              67: b'C',\n",
       "              68: b'D',\n",
       "              69: b'E',\n",
       "              70: b'F',\n",
       "              71: b'G',\n",
       "              72: b'H',\n",
       "              73: b'I',\n",
       "              74: b'J',\n",
       "              75: b'K',\n",
       "              76: b'L',\n",
       "              77: b'M',\n",
       "              78: b'N',\n",
       "              79: b'O',\n",
       "              80: b'P',\n",
       "              81: b'Q',\n",
       "              82: b'R',\n",
       "              83: b'S',\n",
       "              84: b'T',\n",
       "              85: b'U',\n",
       "              86: b'V',\n",
       "              87: b'W',\n",
       "              88: b'X',\n",
       "              89: b'Y',\n",
       "              90: b'Z',\n",
       "              91: b'[',\n",
       "              92: b'\\\\',\n",
       "              93: b']',\n",
       "              94: b'^',\n",
       "              95: b'_',\n",
       "              96: b'`',\n",
       "              97: b'a',\n",
       "              98: b'b',\n",
       "              99: b'c',\n",
       "              100: b'd',\n",
       "              101: b'e',\n",
       "              102: b'f',\n",
       "              103: b'g',\n",
       "              104: b'h',\n",
       "              105: b'i',\n",
       "              106: b'j',\n",
       "              107: b'k',\n",
       "              108: b'l',\n",
       "              109: b'm',\n",
       "              110: b'n',\n",
       "              111: b'o',\n",
       "              112: b'p',\n",
       "              113: b'q',\n",
       "              114: b'r',\n",
       "              115: b's',\n",
       "              116: b't',\n",
       "              117: b'u',\n",
       "              118: b'v',\n",
       "              119: b'w',\n",
       "              120: b'x',\n",
       "              121: b'y',\n",
       "              122: b'z',\n",
       "              123: b'{',\n",
       "              124: b'|',\n",
       "              125: b'}',\n",
       "              126: b'~',\n",
       "              127: b'\\x7f',\n",
       "              128: b'\\x80',\n",
       "              129: b'\\x81',\n",
       "              130: b'\\x82',\n",
       "              131: b'\\x83',\n",
       "              132: b'\\x84',\n",
       "              133: b'\\x85',\n",
       "              134: b'\\x86',\n",
       "              135: b'\\x87',\n",
       "              136: b'\\x88',\n",
       "              137: b'\\x89',\n",
       "              138: b'\\x8a',\n",
       "              139: b'\\x8b',\n",
       "              140: b'\\x8c',\n",
       "              141: b'\\x8d',\n",
       "              142: b'\\x8e',\n",
       "              143: b'\\x8f',\n",
       "              144: b'\\x90',\n",
       "              145: b'\\x91',\n",
       "              146: b'\\x92',\n",
       "              147: b'\\x93',\n",
       "              148: b'\\x94',\n",
       "              149: b'\\x95',\n",
       "              150: b'\\x96',\n",
       "              151: b'\\x97',\n",
       "              152: b'\\x98',\n",
       "              153: b'\\x99',\n",
       "              154: b'\\x9a',\n",
       "              155: b'\\x9b',\n",
       "              156: b'\\x9c',\n",
       "              157: b'\\x9d',\n",
       "              158: b'\\x9e',\n",
       "              159: b'\\x9f',\n",
       "              160: b'\\xa0',\n",
       "              161: b'\\xa1',\n",
       "              162: b'\\xa2',\n",
       "              163: b'\\xa3',\n",
       "              164: b'\\xa4',\n",
       "              165: b'\\xa5',\n",
       "              166: b'\\xa6',\n",
       "              167: b'\\xa7',\n",
       "              168: b'\\xa8',\n",
       "              169: b'\\xa9',\n",
       "              170: b'\\xaa',\n",
       "              171: b'\\xab',\n",
       "              172: b'\\xac',\n",
       "              173: b'\\xad',\n",
       "              174: b'\\xae',\n",
       "              175: b'\\xaf',\n",
       "              176: b'\\xb0',\n",
       "              177: b'\\xb1',\n",
       "              178: b'\\xb2',\n",
       "              179: b'\\xb3',\n",
       "              180: b'\\xb4',\n",
       "              181: b'\\xb5',\n",
       "              182: b'\\xb6',\n",
       "              183: b'\\xb7',\n",
       "              184: b'\\xb8',\n",
       "              185: b'\\xb9',\n",
       "              186: b'\\xba',\n",
       "              187: b'\\xbb',\n",
       "              188: b'\\xbc',\n",
       "              189: b'\\xbd',\n",
       "              190: b'\\xbe',\n",
       "              191: b'\\xbf',\n",
       "              192: b'\\xc0',\n",
       "              193: b'\\xc1',\n",
       "              194: b'\\xc2',\n",
       "              195: b'\\xc3',\n",
       "              196: b'\\xc4',\n",
       "              197: b'\\xc5',\n",
       "              198: b'\\xc6',\n",
       "              199: b'\\xc7',\n",
       "              200: b'\\xc8',\n",
       "              201: b'\\xc9',\n",
       "              202: b'\\xca',\n",
       "              203: b'\\xcb',\n",
       "              204: b'\\xcc',\n",
       "              205: b'\\xcd',\n",
       "              206: b'\\xce',\n",
       "              207: b'\\xcf',\n",
       "              208: b'\\xd0',\n",
       "              209: b'\\xd1',\n",
       "              210: b'\\xd2',\n",
       "              211: b'\\xd3',\n",
       "              212: b'\\xd4',\n",
       "              213: b'\\xd5',\n",
       "              214: b'\\xd6',\n",
       "              215: b'\\xd7',\n",
       "              216: b'\\xd8',\n",
       "              217: b'\\xd9',\n",
       "              218: b'\\xda',\n",
       "              219: b'\\xdb',\n",
       "              220: b'\\xdc',\n",
       "              221: b'\\xdd',\n",
       "              222: b'\\xde',\n",
       "              223: b'\\xdf',\n",
       "              224: b'\\xe0',\n",
       "              225: b'\\xe1',\n",
       "              226: b'\\xe2',\n",
       "              227: b'\\xe3',\n",
       "              228: b'\\xe4',\n",
       "              229: b'\\xe5',\n",
       "              230: b'\\xe6',\n",
       "              231: b'\\xe7',\n",
       "              232: b'\\xe8',\n",
       "              233: b'\\xe9',\n",
       "              234: b'\\xea',\n",
       "              235: b'\\xeb',\n",
       "              236: b'\\xec',\n",
       "              237: b'\\xed',\n",
       "              238: b'\\xee',\n",
       "              239: b'\\xef',\n",
       "              240: b'\\xf0',\n",
       "              241: b'\\xf1',\n",
       "              242: b'\\xf2',\n",
       "              243: b'\\xf3',\n",
       "              244: b'\\xf4',\n",
       "              245: b'\\xf5',\n",
       "              246: b'\\xf6',\n",
       "              247: b'\\xf7',\n",
       "              248: b'\\xf8',\n",
       "              249: b'\\xf9',\n",
       "              250: b'\\xfa',\n",
       "              251: b'\\xfb',\n",
       "              252: b'\\xfc',\n",
       "              253: b'\\xfd',\n",
       "              254: b'\\xfe',\n",
       "              255: b'\\xff'}),\n",
       " 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = defaultdict(bytes)\n",
    "\n",
    "for i in range(256):\n",
    "    vocab[i] = bytes([i])\n",
    "\n",
    "vocab, len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe665f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/Users/jrodriguez/Documentos/personal_projects/cs336/assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ab5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>']\n",
      "b'u'\n",
      "b' don'\n",
      "b\"'t\"\n",
      "b' have'\n",
      "b' to'\n",
      "b' be'\n",
      "b' scared'\n",
      "b' of'\n",
      "b' the'\n",
      "b' loud'\n",
      "b' dog'\n",
      "b','\n",
      "b' I'\n",
      "b\"'ll\"\n",
      "b' protect'\n",
      "b' you'\n",
      "b'\".'\n",
      "b' The'\n",
      "b' mole'\n",
      "b' felt'\n",
      "b' so'\n",
      "b' safe'\n",
      "b' with'\n",
      "b' little'\n",
      "b' girl'\n",
      "b'.'\n",
      "b' She'\n",
      "b' was'\n",
      "b' very'\n",
      "b' kind'\n",
      "b' and'\n",
      "b' soon'\n",
      "b' came'\n",
      "b' trust'\n",
      "b' her'\n",
      "b' He'\n",
      "b' leaned'\n",
      "b' against'\n",
      "b' she'\n",
      "b' kept'\n",
      "b' him'\n",
      "b' had'\n",
      "b' found'\n",
      "b' his'\n",
      "b' best'\n",
      "b' friend'\n",
      "b'\\n'\n",
      "b'\\n'\n",
      "b'Once'\n",
      "b' upon'\n",
      "b' a'\n",
      "b' time'\n",
      "b','\n",
      "b' in'\n",
      "b' warm'\n",
      "b' and'\n",
      "b' sunny'\n",
      "b' place'\n",
      "b' there'\n",
      "b' was'\n",
      "b' big'\n",
      "b' pit'\n",
      "b'.'\n",
      "b' A'\n",
      "b' little'\n",
      "b' boy'\n",
      "b' named'\n",
      "b' Tom'\n",
      "b' liked'\n",
      "b' to'\n",
      "b' play'\n",
      "b' near'\n",
      "b' the'\n",
      "b' One'\n",
      "b' day'\n",
      "b' lost'\n",
      "b' his'\n",
      "b' red'\n",
      "b' ball'\n",
      "b' He'\n",
      "b' very'\n",
      "b' sad'\n",
      "b'Tom'\n",
      "b' asked'\n",
      "b' friend'\n",
      "b' Sam'\n",
      "b' help'\n",
      "b' him'\n",
      "b' search'\n",
      "b' for'\n",
      "b' They'\n",
      "b' looked'\n",
      "b' high'\n",
      "b' low'\n",
      "b' but'\n",
      "b' they'\n",
      "b' could'\n",
      "b' not'\n",
      "b' find'\n",
      "b' said'\n",
      "b' \"'\n",
      "b'I'\n",
      "b' think'\n",
      "b' my'\n",
      "b' fell'\n",
      "b' into'\n",
      "b'.\"'\n",
      "b'Sam'\n",
      "b' went'\n",
      "b' close'\n",
      "b' were'\n",
      "b' scared'\n",
      "b' wanted'\n",
      "b' it'\n",
      "b' too'\n",
      "b' dark'\n",
      "b' see'\n",
      "b'We'\n",
      "b' must'\n",
      "b' go'\n",
      "b'They'\n",
      "b' It'\n",
      "b' scary'\n",
      "b' tried'\n",
      "b' get'\n",
      "b' out'\n",
      "b' deep'\n",
      "b' stuck'\n",
      "b' called'\n",
      "b' no'\n",
      "b' one'\n",
      "b' hear'\n",
      "b' them'\n",
      "b' never'\n",
      "b' got'\n",
      "b' of'\n",
      "b'\\n\\n'\n",
      "b'\\n'\n",
      "b'Tom'\n",
      "b' and'\n",
      "b' Lily'\n",
      "b' were'\n",
      "b' playing'\n",
      "b' with'\n",
      "b' their'\n",
      "b' toys'\n",
      "b' in'\n",
      "b' the'\n",
      "b' living'\n",
      "b' room'\n",
      "b'.'\n",
      "b' They'\n",
      "b' liked'\n",
      "b' to'\n",
      "b' build'\n",
      "b' towers'\n",
      "b' bridges'\n",
      "b' blocks'\n",
      "b' cars'\n",
      "b' Tom'\n",
      "b' was'\n",
      "b' very'\n",
      "b' proud'\n",
      "b' of'\n",
      "b' his'\n",
      "b' tall'\n",
      "b' tower'\n",
      "b' He'\n",
      "b' wanted'\n",
      "b' make'\n",
      "b' it'\n",
      "b' even'\n",
      "b' taller'\n",
      "b','\n",
      "b' so'\n",
      "b' he'\n",
      "b' reached'\n",
      "b' for'\n",
      "b' more'\n",
      "b'\"'\n",
      "b' can'\n",
      "b' I'\n",
      "b' have'\n",
      "b' some'\n",
      "b' too'\n",
      "b'?\"'\n",
      "b' asked'\n",
      "b' She'\n",
      "b' a'\n",
      "b' bridge'\n",
      "b' her'\n",
      "b'No'\n",
      "b' these'\n",
      "b' are'\n",
      "b' mine'\n",
      "b' Go'\n",
      "b' find'\n",
      "b' your'\n",
      "b' own'\n",
      "b',\"'\n",
      "b' said'\n",
      "b' did'\n",
      "b' not'\n",
      "b' want'\n",
      "b' share'\n",
      "b' sister'\n",
      "b' pulled'\n",
      "b' closer'\n",
      "b' him'\n",
      "b'Lily'\n",
      "b' felt'\n",
      "b' sad'\n",
      "b' angry'\n",
      "b' think'\n",
      "b' being'\n",
      "b' nice'\n",
      "b' looked'\n",
      "b' at'\n",
      "b' had'\n",
      "b' an'\n",
      "b' idea'\n",
      "b' decided'\n",
      "b' pull'\n",
      "b' one'\n",
      "b' bottom'\n",
      "b'Suddenly'\n",
      "b' fell'\n",
      "b' down'\n",
      "b' loud'\n",
      "b' crash'\n",
      "b' All'\n",
      "b' scattered'\n",
      "b' on'\n",
      "b' floor'\n",
      "b' shocked'\n",
      "b' shake'\n",
      "b' heard'\n",
      "b' rumble'\n",
      "b' It'\n",
      "b' earthquake'\n",
      "b'!'\n",
      "b'Mommy'\n",
      "b' Daddy'\n",
      "b'!\"'\n",
      "b' they'\n",
      "b' cried'\n",
      "b' scared'\n",
      "b' ran'\n",
      "b' parents'\n",
      "b' who'\n",
      "b' kitchen'\n",
      "b'Are'\n",
      "b' you'\n",
      "b' okay'\n",
      "b' kids'\n",
      "b' Mommy'\n",
      "b' hugged'\n",
      "b' them'\n",
      "b' checked'\n",
      "b' if'\n",
      "b' hurt'\n",
      "b'We'\n",
      "b\"'re\"\n",
      "b' But'\n",
      "b' our'\n",
      "b' broken'\n",
      "b'I'\n",
      "b\"'m\"\n",
      "b' sorry'\n",
      "b' important'\n",
      "b' You'\n",
      "b' We'\n",
      "b' safe'\n",
      "b' together'\n",
      "b' That'\n",
      "b\"'s\"\n",
      "b' what'\n",
      "b' matters'\n",
      "b' realized'\n",
      "b' selfish'\n",
      "b' mean'\n",
      "b' saw'\n",
      "b' how'\n",
      "b' she'\n",
      "b' during'\n",
      "b' happy'\n",
      "b' all'\n",
      "b' love'\n",
      "b' smiled'\n",
      "b' forgave'\n",
      "b' thanked'\n",
      "b' loved'\n",
      "b'They'\n",
      "b' went'\n",
      "b' back'\n",
      "b' cleaned'\n",
      "b' up'\n",
      "b' something'\n",
      "b' made'\n",
      "b' big'\n",
      "b' house'\n",
      "b' garden'\n",
      "b' fence'\n",
      "b' put'\n",
      "b' dolls'\n",
      "b' inside'\n",
      "b' work'\n",
      "b' came'\n",
      "b' see'\n",
      "b' praised'\n",
      "b' gave'\n",
      "b' treat'\n",
      "b' lemon'\n",
      "b' cake'\n",
      "b' sour'\n",
      "b' but'\n",
      "b' learned'\n",
      "b' that'\n",
      "b' sharing'\n",
      "b' is'\n",
      "b' caring'\n",
      "b' family'\n",
      "b' sweet'\n",
      "b'\\n'\n",
      "b'Once'\n",
      "b' upon'\n",
      "b' a'\n",
      "b' time'\n",
      "b' there'\n",
      "b' was'\n",
      "b' little'\n",
      "b' girl'\n",
      "b' named'\n",
      "b' Lucy'\n",
      "b'.'\n",
      "b' She'\n",
      "b' loved'\n",
      "b' to'\n",
      "b' go'\n",
      "b' the'\n",
      "b' store'\n",
      "b' buy'\n",
      "b' sweets'\n",
      "b' with'\n",
      "b' her'\n",
      "b' mom'\n",
      "b' and'\n",
      "b' dad'\n",
      "b' On'\n",
      "b' this'\n",
      "b' special'\n",
      "b' day'\n",
      "b','\n",
      "b' entered'\n",
      "b' feeling'\n",
      "b' so'\n",
      "b' excited'\n",
      "b'As'\n",
      "b' they'\n",
      "b' were'\n",
      "b' looking'\n",
      "b' around'\n",
      "b' noticed'\n",
      "b' playing'\n",
      "b' toy'\n",
      "b' in'\n",
      "b' corner'\n",
      "b' of'\n",
      "b' gasped'\n",
      "b' excitement'\n",
      "b' ran'\n",
      "b' towards'\n",
      "b' asked'\n",
      "b' if'\n",
      "b' she'\n",
      "b' could'\n",
      "b' play'\n",
      "b' too'\n",
      "b' but'\n",
      "b' said'\n",
      "b' no'\n",
      "b' rather'\n",
      "b' grumpy'\n",
      "b' not'\n",
      "b' mood'\n",
      "b'Lucy'\n",
      "b\"'s\"\n",
      "b' saw'\n",
      "b' what'\n",
      "b' going'\n",
      "b' on'\n",
      "b' told'\n",
      "b' \"'\n",
      "b'Let'\n",
      "b' try'\n",
      "b' be'\n",
      "b' peaceful'\n",
      "b' kind'\n",
      "b' Have'\n",
      "b' patience'\n",
      "b' understanding'\n",
      "b' Together'\n",
      "b' you'\n",
      "b' can'\n",
      "b' both'\n",
      "b' happy'\n",
      "b'!\"'\n",
      "b'So'\n",
      "b' smiled'\n",
      "b' at'\n",
      "b'Can'\n",
      "b' we'\n",
      "b' together'\n",
      "b'?\"'\n",
      "b' The'\n",
      "b' softened'\n",
      "b' back'\n",
      "b' agreed'\n",
      "b' share'\n",
      "b' even'\n",
      "b' let'\n",
      "b' have'\n",
      "b' turn'\n",
      "b' first'\n",
      "b' played'\n",
      "b' happily'\n",
      "b' In'\n",
      "b' end'\n",
      "b' learnt'\n",
      "b' an'\n",
      "b' important'\n",
      "b' lesson'\n",
      "b':'\n",
      "b' when'\n",
      "b' faced'\n",
      "b' conflict'\n",
      "b' And'\n",
      "b' that'\n",
      "b' is'\n",
      "b' why'\n",
      "b' became'\n",
      "b' great'\n",
      "b' friends'\n",
      "b'\\n'\n",
      "b'One'\n",
      "b' morning'\n",
      "b','\n",
      "b' a'\n",
      "b' cat'\n",
      "b' named'\n",
      "b' Tom'\n",
      "b' woke'\n",
      "b' up'\n",
      "b'.'\n",
      "b' He'\n",
      "b' felt'\n",
      "b' happy'\n",
      "b' because'\n",
      "b' the'\n",
      "b' sun'\n",
      "b' was'\n",
      "b' shining'\n",
      "b' wanted'\n",
      "b' to'\n",
      "b' start'\n",
      "b' his'\n",
      "b' day'\n",
      "b' so'\n",
      "b' he'\n",
      "b' did'\n",
      "b' big'\n",
      "b' stretch'\n",
      "b' stretched'\n",
      "b' legs'\n",
      "b' back'\n",
      "b' and'\n",
      "b' tail'\n",
      "b' It'\n",
      "b' easy'\n",
      "b' good'\n",
      "b'Tom'\n",
      "b' went'\n",
      "b' outside'\n",
      "b' play'\n",
      "b' saw'\n",
      "b' friend'\n",
      "b' dog'\n",
      "b' Max'\n",
      "b' also'\n",
      "b' stretching'\n",
      "b' in'\n",
      "b' They'\n",
      "b' both'\n",
      "b' very'\n",
      "b' decided'\n",
      "b' together'\n",
      "b' have'\n",
      "b' fun'\n",
      "b' all'\n",
      "b'At'\n",
      "b' end'\n",
      "b' of'\n",
      "b' were'\n",
      "b' tired'\n",
      "b' had'\n",
      "b' played'\n",
      "b' lots'\n",
      "b' said'\n",
      "b' goodbye'\n",
      "b' each'\n",
      "b' other'\n",
      "b' their'\n",
      "b' homes'\n",
      "b' Before'\n",
      "b' going'\n",
      "b' sleep'\n",
      "b' they'\n",
      "b' another'\n",
      "b' knew'\n",
      "b' that'\n",
      "b' tomorrow'\n",
      "b' would'\n",
      "b' be'\n",
      "b'u'\n",
      "b' don'\n",
      "b\"'t\"\n",
      "b' have'\n",
      "b' to'\n",
      "b' be'\n",
      "b' scared'\n",
      "b' of'\n",
      "b' the'\n",
      "b' loud'\n",
      "b' dog'\n",
      "b','\n",
      "b' I'\n",
      "b\"'ll\"\n",
      "b' protect'\n",
      "b' you'\n",
      "b'\".'\n",
      "b' The'\n",
      "b' mole'\n",
      "b' felt'\n",
      "b' so'\n",
      "b' safe'\n",
      "b' with'\n",
      "b' little'\n",
      "b' girl'\n",
      "b'.'\n",
      "b' She'\n",
      "b' was'\n",
      "b' very'\n",
      "b' kind'\n",
      "b' and'\n",
      "b' soon'\n",
      "b' came'\n",
      "b' trust'\n",
      "b' her'\n",
      "b' He'\n",
      "b' leaned'\n",
      "b' against'\n",
      "b' she'\n",
      "b' kept'\n",
      "b' him'\n",
      "b' had'\n",
      "b' found'\n",
      "b' his'\n",
      "b' best'\n",
      "b' friend'\n",
      "b'\\n'\n",
      "b'\\n'\n",
      "b'Once'\n",
      "b' upon'\n",
      "b' a'\n",
      "b' time'\n",
      "b','\n",
      "b' in'\n",
      "b' warm'\n",
      "b' and'\n",
      "b' sunny'\n",
      "b' place'\n",
      "b' there'\n",
      "b' was'\n",
      "b' big'\n",
      "b' pit'\n",
      "b'.'\n",
      "b' A'\n",
      "b' little'\n",
      "b' boy'\n",
      "b' named'\n",
      "b' Tom'\n",
      "b' liked'\n",
      "b' to'\n",
      "b' play'\n",
      "b' near'\n",
      "b' the'\n",
      "b' One'\n",
      "b' day'\n",
      "b' lost'\n",
      "b' his'\n",
      "b' red'\n",
      "b' ball'\n",
      "b' He'\n",
      "b' very'\n",
      "b' sad'\n",
      "b'Tom'\n",
      "b' asked'\n",
      "b' friend'\n",
      "b' Sam'\n",
      "b' help'\n",
      "b' him'\n",
      "b' search'\n",
      "b' for'\n",
      "b' They'\n",
      "b' looked'\n",
      "b' high'\n",
      "b' low'\n",
      "b' but'\n",
      "b' they'\n",
      "b' could'\n",
      "b' not'\n",
      "b' find'\n",
      "b' said'\n",
      "b' \"'\n",
      "b'I'\n",
      "b' think'\n",
      "b' my'\n",
      "b' fell'\n",
      "b' into'\n",
      "b'.\"'\n",
      "b'Sam'\n",
      "b' went'\n",
      "b' close'\n",
      "b' were'\n",
      "b' scared'\n",
      "b' wanted'\n",
      "b' it'\n",
      "b' too'\n",
      "b' dark'\n",
      "b' see'\n",
      "b'We'\n",
      "b' must'\n",
      "b' go'\n",
      "b'They'\n",
      "b' It'\n",
      "b' scary'\n",
      "b' tried'\n",
      "b' get'\n",
      "b' out'\n",
      "b' deep'\n",
      "b' stuck'\n",
      "b' called'\n",
      "b' no'\n",
      "b' one'\n",
      "b' hear'\n",
      "b' them'\n",
      "b' never'\n",
      "b' got'\n",
      "b' of'\n",
      "b'\\n\\n'\n",
      "b'\\n'\n",
      "b'Tom'\n",
      "b' and'\n",
      "b' Lily'\n",
      "b' were'\n",
      "b' playing'\n",
      "b' with'\n",
      "b' their'\n",
      "b' toys'\n",
      "b' in'\n",
      "b' the'\n",
      "b' living'\n",
      "b' room'\n",
      "b'.'\n",
      "b' They'\n",
      "b' liked'\n",
      "b' to'\n",
      "b' build'\n",
      "b' towers'\n",
      "b' bridges'\n",
      "b' blocks'\n",
      "b' cars'\n",
      "b' Tom'\n",
      "b' was'\n",
      "b' very'\n",
      "b' proud'\n",
      "b' of'\n",
      "b' his'\n",
      "b' tall'\n",
      "b' tower'\n",
      "b' He'\n",
      "b' wanted'\n",
      "b' make'\n",
      "b' it'\n",
      "b' even'\n",
      "b' taller'\n",
      "b','\n",
      "b' so'\n",
      "b' he'\n",
      "b' reached'\n",
      "b' for'\n",
      "b' more'\n",
      "b'\"'\n",
      "b' can'\n",
      "b' I'\n",
      "b' have'\n",
      "b' some'\n",
      "b' too'\n",
      "b'?\"'\n",
      "b' asked'\n",
      "b' She'\n",
      "b' a'\n",
      "b' bridge'\n",
      "b' her'\n",
      "b'No'\n",
      "b' these'\n",
      "b' are'\n",
      "b' mine'\n",
      "b' Go'\n",
      "b' find'\n",
      "b' your'\n",
      "b' own'\n",
      "b',\"'\n",
      "b' said'\n",
      "b' did'\n",
      "b' not'\n",
      "b' want'\n",
      "b' share'\n",
      "b' sister'\n",
      "b' pulled'\n",
      "b' closer'\n",
      "b' him'\n",
      "b'Lily'\n",
      "b' felt'\n",
      "b' sad'\n",
      "b' angry'\n",
      "b' think'\n",
      "b' being'\n",
      "b' nice'\n",
      "b' looked'\n",
      "b' at'\n",
      "b' had'\n",
      "b' an'\n",
      "b' idea'\n",
      "b' decided'\n",
      "b' pull'\n",
      "b' one'\n",
      "b' bottom'\n",
      "b'Suddenly'\n",
      "b' fell'\n",
      "b' down'\n",
      "b' loud'\n",
      "b' crash'\n",
      "b' All'\n",
      "b' scattered'\n",
      "b' on'\n",
      "b' floor'\n",
      "b' shocked'\n",
      "b' shake'\n",
      "b' heard'\n",
      "b' rumble'\n",
      "b' It'\n",
      "b' earthquake'\n",
      "b'!'\n",
      "b'Mommy'\n",
      "b' Daddy'\n",
      "b'!\"'\n",
      "b' they'\n",
      "b' cried'\n",
      "b' scared'\n",
      "b' ran'\n",
      "b' parents'\n",
      "b' who'\n",
      "b' kitchen'\n",
      "b'Are'\n",
      "b' you'\n",
      "b' okay'\n",
      "b' kids'\n",
      "b' Mommy'\n",
      "b' hugged'\n",
      "b' them'\n",
      "b' checked'\n",
      "b' if'\n",
      "b' hurt'\n",
      "b'We'\n",
      "b\"'re\"\n",
      "b' But'\n",
      "b' our'\n",
      "b' broken'\n",
      "b'I'\n",
      "b\"'m\"\n",
      "b' sorry'\n",
      "b' important'\n",
      "b' You'\n",
      "b' We'\n",
      "b' safe'\n",
      "b' together'\n",
      "b' That'\n",
      "b\"'s\"\n",
      "b' what'\n",
      "b' matters'\n",
      "b' realized'\n",
      "b' selfish'\n",
      "b' mean'\n",
      "b' saw'\n",
      "b' how'\n",
      "b' she'\n",
      "b' during'\n",
      "b' happy'\n",
      "b' all'\n",
      "b' love'\n",
      "b' smiled'\n",
      "b' forgave'\n",
      "b' thanked'\n",
      "b' loved'\n",
      "b'They'\n",
      "b' went'\n",
      "b' back'\n",
      "b' cleaned'\n",
      "b' up'\n",
      "b' something'\n",
      "b' made'\n",
      "b' big'\n",
      "b' house'\n",
      "b' garden'\n",
      "b' fence'\n",
      "b' put'\n",
      "b' dolls'\n",
      "b' inside'\n",
      "b' work'\n",
      "b' came'\n",
      "b' see'\n",
      "b' praised'\n",
      "b' gave'\n",
      "b' treat'\n",
      "b' lemon'\n",
      "b' cake'\n",
      "b' sour'\n",
      "b' but'\n",
      "b' learned'\n",
      "b' that'\n",
      "b' sharing'\n",
      "b' is'\n",
      "b' caring'\n",
      "b' family'\n",
      "b' sweet'\n",
      "b'\\n'\n",
      "b'Once'\n",
      "b' upon'\n",
      "b' a'\n",
      "b' time'\n",
      "b' there'\n",
      "b' was'\n",
      "b' little'\n",
      "b' girl'\n",
      "b' named'\n",
      "b' Lucy'\n",
      "b'.'\n",
      "b' She'\n",
      "b' loved'\n",
      "b' to'\n",
      "b' go'\n",
      "b' the'\n",
      "b' store'\n",
      "b' buy'\n",
      "b' sweets'\n",
      "b' with'\n",
      "b' her'\n",
      "b' mom'\n",
      "b' and'\n",
      "b' dad'\n",
      "b' On'\n",
      "b' this'\n",
      "b' special'\n",
      "b' day'\n",
      "b','\n",
      "b' entered'\n",
      "b' feeling'\n",
      "b' so'\n",
      "b' excited'\n",
      "b'As'\n",
      "b' they'\n",
      "b' were'\n",
      "b' looking'\n",
      "b' around'\n",
      "b' noticed'\n",
      "b' playing'\n",
      "b' toy'\n",
      "b' in'\n",
      "b' corner'\n",
      "b' of'\n",
      "b' gasped'\n",
      "b' excitement'\n",
      "b' ran'\n",
      "b' towards'\n",
      "b' asked'\n",
      "b' if'\n",
      "b' she'\n",
      "b' could'\n",
      "b' play'\n",
      "b' too'\n",
      "b' but'\n",
      "b' said'\n",
      "b' no'\n",
      "b' rather'\n",
      "b' grumpy'\n",
      "b' not'\n",
      "b' mood'\n",
      "b'Lucy'\n",
      "b\"'s\"\n",
      "b' saw'\n",
      "b' what'\n",
      "b' going'\n",
      "b' on'\n",
      "b' told'\n",
      "b' \"'\n",
      "b'Let'\n",
      "b' try'\n",
      "b' be'\n",
      "b' peaceful'\n",
      "b' kind'\n",
      "b' Have'\n",
      "b' patience'\n",
      "b' understanding'\n",
      "b' Together'\n",
      "b' you'\n",
      "b' can'\n",
      "b' both'\n",
      "b' happy'\n",
      "b'!\"'\n",
      "b'So'\n",
      "b' smiled'\n",
      "b' at'\n",
      "b'Can'\n",
      "b' we'\n",
      "b' together'\n",
      "b'?\"'\n",
      "b' The'\n",
      "b' softened'\n",
      "b' back'\n",
      "b' agreed'\n",
      "b' share'\n",
      "b' even'\n",
      "b' let'\n",
      "b' have'\n",
      "b' turn'\n",
      "b' first'\n",
      "b' played'\n",
      "b' happily'\n",
      "b' In'\n",
      "b' end'\n",
      "b' learnt'\n",
      "b' an'\n",
      "b' important'\n",
      "b' lesson'\n",
      "b':'\n",
      "b' when'\n",
      "b' faced'\n",
      "b' conflict'\n",
      "b' And'\n",
      "b' that'\n",
      "b' is'\n",
      "b' why'\n",
      "b' became'\n",
      "b' great'\n",
      "b' friends'\n",
      "b'\\n'\n",
      "b'One'\n",
      "b' morning'\n",
      "b','\n",
      "b' a'\n",
      "b' cat'\n",
      "b' named'\n",
      "b' Tom'\n",
      "b' woke'\n",
      "b' up'\n",
      "b'.'\n",
      "b' He'\n",
      "b' felt'\n",
      "b' happy'\n",
      "b' because'\n",
      "b' the'\n",
      "b' sun'\n",
      "b' was'\n",
      "b' shining'\n",
      "b' wanted'\n",
      "b' to'\n",
      "b' start'\n",
      "b' his'\n",
      "b' day'\n",
      "b' so'\n",
      "b' he'\n",
      "b' did'\n",
      "b' big'\n",
      "b' stretch'\n",
      "b' stretched'\n",
      "b' legs'\n",
      "b' back'\n",
      "b' and'\n",
      "b' tail'\n",
      "b' It'\n",
      "b' easy'\n",
      "b' good'\n",
      "b'Tom'\n",
      "b' went'\n",
      "b' outside'\n",
      "b' play'\n",
      "b' saw'\n",
      "b' friend'\n",
      "b' dog'\n",
      "b' Max'\n",
      "b' also'\n",
      "b' stretching'\n",
      "b' in'\n",
      "b' They'\n",
      "b' both'\n",
      "b' very'\n",
      "b' decided'\n",
      "b' together'\n",
      "b' have'\n",
      "b' fun'\n",
      "b' all'\n",
      "b'At'\n",
      "b' end'\n",
      "b' of'\n",
      "b' were'\n",
      "b' tired'\n",
      "b' had'\n",
      "b' played'\n",
      "b' lots'\n",
      "b' said'\n",
      "b' goodbye'\n",
      "b' each'\n",
      "b' other'\n",
      "b' their'\n",
      "b' homes'\n",
      "b' Before'\n",
      "b' going'\n",
      "b' sleep'\n",
      "b' they'\n",
      "b' another'\n",
      "b' knew'\n",
      "b' that'\n",
      "b' tomorrow'\n",
      "b' would'\n",
      "b' be'\n"
     ]
    }
   ],
   "source": [
    "from email.policy import default\n",
    "\n",
    "\n",
    "def merge(seq_bytes: bytes):\n",
    "    \n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for byte_1, byte_2 in zip(seq_bytes[:-1], seq_bytes[1:]):\n",
    "        counts[(byte_1, byte_2)] += 1\n",
    "\n",
    "def get_max_pair(counts: dict[tuple[int, int], int], vocab: dict[int, bytes]):\n",
    "    pairs = max(counts, key=counts.get)\n",
    "\n",
    "    if isinstance(pairs, tuple):\n",
    "        return pairs\n",
    "    elif isinstance(pairs, list):  # Get the greater lexicographical pair\n",
    "        # Get each pair in string\n",
    "        string_pairs = []\n",
    "        string_2_int = {}\n",
    "        for pair in pairs:\n",
    "            string_1 = vocab[pair[0]].decode()\n",
    "            string_2 = vocab[pair[1]].decode()\n",
    "            string_2_int[(string_1, string_2)] = pair\n",
    "\n",
    "            string_pairs.append((string_1, string_2))\n",
    "\n",
    "        max_string_pair = max(string_pairs)\n",
    "\n",
    "        return string_2_int[max_string_pair]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Function to train the byte-level BPE tokenizer\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to file where the BPE should be trained.\n",
    "        vocab_size (int): The maximum vocabulary size including the initial ASCII character bytes, the special tokens and the merge generated tokens.\n",
    "        special_tokens (list[str]): A list with the special tokens of our tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        vocab (dict[int, bytes]): The vocabulary of our tokenizer.\n",
    "        merges (dict[tuple[int, int], int]): The merges resulted from the training.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf_8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Get and remove special tokens before pre-tokenization\n",
    "    pattern = \"|\".join([re.escape(special_token) for special_token in special_tokens])\n",
    "    split_text = re.split(pattern, text)\n",
    "\n",
    "    # Init vocab with ASCII character bytes and special tokens\n",
    "    vocab = defaultdict(bytes)\n",
    "    for i in range(256):\n",
    "        vocab[i] = bytes([i])\n",
    "    for special_token in special_tokens:\n",
    "        vocab[len(vocab)] = special_token.encode(encoding=\"utf-8\")\n",
    "\n",
    "    # Init merges\n",
    "    merges = []\n",
    "\n",
    "    # Pre-tokenize and obtain the initial pretoken_counts\n",
    "    for small_text in split_text[:5]:\n",
    "            if len(small_text) == 0:\n",
    "                continue\n",
    "\n",
    "            # Pre-tokenize text and count pre-tojens\n",
    "            pretoken_counts = defaultdict(int)  # dict[str, int]\n",
    "            for match in re.finditer(PAT, small_text):\n",
    "                string = match.group(0)\n",
    "                indices = list(map(int, string.encode(encoding=\"utf-8\")))\n",
    "                pretoken_counts[indices] += 1\n",
    "\n",
    "\n",
    "    # Merge pairs until you reach vocab_size vocabulary size\n",
    "    while len(vocab) < vocab_size:\n",
    "        counts = defaultdict(int)\n",
    "        for small_text in split_text[:5]:\n",
    "            if len(small_text) == 0:\n",
    "                continue\n",
    "\n",
    "            # Pre-tokenize text and count pre-tojens\n",
    "            pretoken_counts = defaultdict(int)  # dict[str, int]\n",
    "            for match in re.finditer(PAT, small_text):\n",
    "                string = match.group(0)\n",
    "                pretoken_counts[string.encode(encoding=\"utf-8\")] += 1\n",
    "\n",
    "            \n",
    "            for pretoken, num_appaerances in pretoken_counts.items():\n",
    "                byte_pair_count = count_byte_pairs(pretoken, num_appaerances)\n",
    "                for byte_pair, count in byte_pair_count.items():\n",
    "                    counts[byte_pair] += count\n",
    "\n",
    "        \n",
    "        # Find the most common byte pair and merge\n",
    "        pair = get_max_pair(counts, vocab)\n",
    "        index_1, index_2 = pair\n",
    "        bytes_tuple = vocab[index_1], vocab[index_2]\n",
    "        merges.append(bytes_tuple)\n",
    "        vocab[len(vocab)] = bytes_tuple[0] + bytes_tuple[1]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "print(special_tokens)       \n",
    "train_bpe(input_path, vocab_size=1000, special_tokens=special_tokens)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0931aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B', 'ZZ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pairs = (\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\")\n",
    "\n",
    "max(max_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0392faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<\\\\|endoftext\\\\|>|\\\\\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read text and split the remove special tokens by spliting the text with them\n",
    "# NOTE: Later, after each splitted text, you will have to add the special tokens found \n",
    "input_path = \"/Users/jrodriguez/Documentos/personal_projects/cs336/assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "special_tokens = [re.escape(\"<|endoftext|>\"), re.escape(\"\\n\")]\n",
    "pattern = \"|\".join(special_tokens)\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e54f14ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"u don't ha\", '', '', 'Once upon ', 'Tom asked ', 'Sam and To', 'They went ')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(input_path, \"r\", encoding=\"utf_8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "    split_text = re.split(pattern, text)\n",
    "\n",
    "\n",
    "split_text[0][:10], split_text[1][:10], split_text[2][:10], split_text[3][:10], split_text[4][:10], split_text[5][:10], split_text[6][:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7d411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
